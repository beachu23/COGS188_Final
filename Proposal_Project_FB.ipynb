{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "Which Reinforcement Learning Is Best in a 3D Maze? \n",
    "We propose a Reinforcement Learning (RL) project in which an agent navigates a nontrivial environment that includes:\n",
    "Multi-dimensional grids (starting with 2D, potentially extending to 3D).\n",
    "Moving obstacles or “shifting walls.”\n",
    "Slips that might act as portals sending the agent to random or fixed distant cells.\n",
    "Mines that terminate an episode on contact.\n",
    "\n",
    "\n",
    "We will implement three RL methods—Dynamic Programming (DP), Monte Carlo (MC), and Temporal Difference (TD)—and compare their performance in terms of convergence rate, policy quality, and adaptability to environment complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Fabian Tellez (A18095006)\n",
    "- Brandon Chu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "In this project, we go past the basic grid world by incorporating features such as moving obstacles, portal-like slips, and mines. The agent must learn a policy to complete the maze efficiently from a start region to a goal region while avoiding catastrophic states (mines), handling teleportations (portals), and contending with shifting barriers. We use three RL methods, DP, MC, and TD, to see how each manages these increasingly complex dynamics. Our project will cover convergence speed, average cumulative reward, path optimality, and stability across multiple environment scales (2D up to 3D). A comparison between a model-based approach (DP) and model-free sampling (MC, TD) gives insight into sample efficiency, computational requirements, and responsiveness to environmental change. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 3 to 5 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Reinforcement Learning is an approach in AI algorithms that attempts to train an agent through trial and error. This trial and error results in reward points or penalty points depending on if the action is \"good\" or \"bad.\" Usually, RL methods use static grids or low dimensional states<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). This approach does not really work in a real-world setting since the real world is filled with dynamic and stochastic factors.\n",
    "\n",
    "- Model-Based: DP is a model-based algorithm that needs complete knowledge of the state transitions but eventually converges when in stable conditions.\n",
    "\n",
    "- Model Free: Monte Carlon and TD methods like Q-Learning are model-free algorithms that do not need to know explicit transitions since they adapt and learn from their experience in the environment<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). \n",
    "\n",
    "Studies in the past have explored certain complex factors like random slips or partial observability, but they're usually done in static or 2D grids or both. The project pushes the complexity a bit further by implementing moving obstacles, portal events, mines, and potential 3D expansions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The main objective of the project is to learn the most efficient policy in a dynamic environment that may include:\n",
    "- Random portal slips (teleportation) are used to bring in randomness.\n",
    "- moving obstacles that move with each agent's step to test how a model-based approach does.\n",
    "- mines that will end an iteration immediately and result in -10 pts. \n",
    "- scaling the grid up from a 2D to a 3D grid (10x10x10)\n",
    "- We will be able to measure the states which will either be (x,y) or (x,y,z) and the actions will include up, down, left, right. If 3D is picked, there will also be a positive and negative z direction.\n",
    "- We will also measure the total reward points accumilated by the agent, steps taken, success rates or the percent of the time the agent avoids mines. \n",
    "- It should also be replicable since we can generate a grid with random seeds allowing us to recreate the environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Data will be generated locally and artificially:\n",
    "- The data includes the grid (10x10) or (10x10x10).\n",
    "- the starting point.\n",
    "- The state space (x,y,z?).\n",
    "- Transitions, including moving obstacles, portal slips, mines, and the goal. \n",
    "- The total reward points accumulated by the agent range from a small penalty (-.01) to a large penalty for stepping on a mine (-10) and +1 for reaching the goal. \n",
    "- How does a more complex environment affect the algorithms and avoid a trivial solution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Dynamic Programming (DP):\n",
    "- The two approaches being considered are value iteration and policy iteration.\n",
    "- Moving obstacles might present a problem for DP., so in order for it to work around that issue, we can treat time or obstacle positions as a part of the extended state. We could also just accept the partial nonstationarity. \n",
    "\n",
    "Monte Carlo (MC): \n",
    "- Model-free approach.\n",
    "- Every visit or first visit method uses the Epsilon greedy policy.\n",
    "- Learns from the returns it gets and adapts to environments if enough iterations.\n",
    "\n",
    "Temporal Difference (TD):\n",
    "- Q-Learning or SARSA\n",
    "- It will update Q values and add them up.\n",
    "- We will compare learning rates (alpha), discount (gamma), and epsilon greedy for possible different strategies regarding performance differences.\n",
    "\n",
    "Implementation Plan:\n",
    "- We will start with a static 2D environment.\n",
    "- After that, we will introduce features like mines, portals, and moving obstacles. \n",
    "- We might scale the grid up to 3D if viable. \n",
    "\n",
    "Benchmark:\n",
    "- We can use DP to create a baseline that can be generated from a static grid. \n",
    "- Evaluate how closely MC and TD approaches approximate under different complex features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "1. Convergence Speed: \n",
    "    - Number of sweeps until the returns stabilize\n",
    "    - For both MC and TD, we can count how many episodes or total steps the agents take until the policy is optimal till 95%. \n",
    "\n",
    "2. Policy Quality:\n",
    "    - This compares the final policy with a baseline. The baseline comes from path length or total reward points accumulated.\n",
    "    - For moving obstacles, measure agent success rate or average negative reward points. \n",
    "\n",
    "3. Complexity \n",
    "    - Change the obstacle movement, slip portal probability, or change the grid to 3D. \n",
    "    - Track how each method either improves or gets worse. \n",
    "\n",
    "4. Stability\n",
    "    - The standard deviation can be a measure of the returns or success rate across many different grid seeds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a synthetic environment so no personal data is collected. \n",
    "There are also no privacy issues since we are not using any person's real time information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* *Communicate*\n",
    "* *Put in effort*\n",
    "* *Distribute work Evenly*\n",
    "* *Adhere to timelines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 02/14  |  11:59 PM |  Turn in Project Proposal  |Turn in Project Proposal | \n",
    "| 02/22  |  11:59 PM |  Look over feedback | Brainstorm Feedback | \n",
    "| 03/01  | 11:59 PM  | Start implementing project ideas  | Work together to start project|\n",
    "| 03/08  | 11:59 PM  | Work on project | Touch base with team and see where we stand|\n",
    "| 03/15  | 11:59 PM | Work on project | Touch base with team and see where we stand |\n",
    "| 03/17  | 11:59 PM  | Work on project| Touch base with team and see where we stand |\n",
    "| 03/19  | Before 11:59 PM  | Finalize finishing details | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Sutton1\"></a>1. Sutton, R.S., & Barto. A.G. \"Dynamic Programming\" Reinforcement Learning: An Introduction. *MIT Press*, 2018. PDF file: 05_DynamicProgramming.pdf. <br> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
