{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "#                      VARIABLES FOR EXPERIMENTATION\n",
    "###############################################################################\n",
    "# --- ENVIRONMENT SETTINGS ---\n",
    "ENV_SIZE = (10, 10, 10)       # (x_size, y_size, z_size). Increase for a bigger 3D grid; \n",
    "                              # decreasing them makes for a smaller state space easier to debug.\n",
    "\n",
    "OBSTACLES_COUNT = 15         # How many random obstacles to generate. \n",
    "                             # More obstacles => less open space, more challenge.\n",
    "\n",
    "MINES_COUNT = 5              # Number of random mines. \n",
    "                             # Increases the risk of sudden negative rewards.\n",
    "\n",
    "SLIP_P = 0.2                 # Probability of a random slip/portal event. \n",
    "                             # 0 => no slip, 1 => always slip. Higher makes environment more stochastic.\n",
    "\n",
    "SLIP_MODE = \"random\"         # \"random\" => slip to a random free cell; \n",
    "                             # \"fixed\" => slip to a chosen 'portal_exit'.\n",
    "\n",
    "PORTAL_EXIT = (0,0,0)        # Only matters if SLIP_MODE = 'fixed'. \n",
    "                             # The cell to which you teleport if slip occurs.\n",
    "\n",
    "MOVING_OBSTACLES = True      # If True, obstacles shift each step (nonstationary environment). \n",
    "                             # Set to False to keep obstacles stationary.\n",
    "\n",
    "STEP_PENALTY = -0.01         # Negative reward per step/time step. \n",
    "                             # Encourages faster solutions.\n",
    "\n",
    "MINE_PENALTY = -10.0         # Negative reward for hitting a mine. \n",
    "                             # Increase magnitude to punish stepping on mines more severely.\n",
    "\n",
    "START_STATE = (0,0,0)        # Where the agent starts\n",
    "GOAL_STATE  = (9,9,9)        # The goal location in the 3D grid\n",
    "\n",
    "# --- RL SETTINGS ---\n",
    "GAMMA = 0.9                  # Discount factor for Value Iteration, MC, Q-learning \n",
    "THETA = 1e-3                 # Convergence threshold for Value Iteration\n",
    "MAX_ITERS_VALUE_ITER = 60    # Max iterations for Value Iteration\n",
    "SAMPLES_VALUE_ITER = 20      # Number of samples used to approximate p(s'|s,a) in Value Iteration\n",
    "\n",
    "# Monte Carlo\n",
    "MC_EPISODES = 4000           # Number of training episodes for the Monte Carlo approach\n",
    "MC_EPSILON  = 0.15           # Epsilon-greedy factor for MC\n",
    "\n",
    "# Q-Learning\n",
    "QL_EPISODES   = 4000         # Number of training episodes for Q-learning\n",
    "QL_ALPHA      = 0.1          # Learning rate\n",
    "QL_EPSILON    = 0.15         # Epsilon-greedy factor\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "#                          FANCY 3D GRID ENVIRONMENT\n",
    "###############################################################################\n",
    "class Fancy3DGridEnv:\n",
    "    \"\"\"\n",
    "    Fancy3DGridEnv \n",
    "    --------------\n",
    "    3D environment with:\n",
    "      - random/fixed obstacles,\n",
    "      - random mines,\n",
    "      - slip probability for portal teleports,\n",
    "      - moving obstacles,\n",
    "      - step penalties,\n",
    "      - negative reward for mines,\n",
    "      - single (start) -> single (goal).\n",
    "\n",
    "    PRIMARY VARIABLES IN-USE:\n",
    "      - x_size, y_size, z_size (ENV_SIZE)\n",
    "      - obstacles_count, mines_count (or direct sets)\n",
    "      - slip_p, slip_mode, portal_exit\n",
    "      - moving_obstacles\n",
    "      - step_penalty, mine_penalty\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_size=10,\n",
    "        y_size=10,\n",
    "        z_size=10,\n",
    "        start_state=(0,0,0),\n",
    "        goal_state=(9,9,9),\n",
    "        obstacles=None,\n",
    "        mines=None,\n",
    "        slip_p=0.2,\n",
    "        slip_mode='random',\n",
    "        portal_exit=(0,0,0),\n",
    "        moving_obstacles=True,\n",
    "        step_penalty=-0.01,\n",
    "        mine_penalty=-10.0,\n",
    "        obstacles_count=10,\n",
    "        mines_count=5\n",
    "    ):\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.z_size = z_size\n",
    "        \n",
    "        self.start_state = start_state\n",
    "        self.goal_state  = goal_state\n",
    "        \n",
    "        # If no obstacles specified, we randomly create obstacles_count of them\n",
    "        if obstacles is None:\n",
    "            obstacles = self._generate_random_obstacles(obstacles_count)\n",
    "        self.obstacles = set(obstacles)\n",
    "        \n",
    "        # If no mines specified, create mines_count randomly\n",
    "        if mines is None:\n",
    "            mines = self._generate_random_mines(mines_count)\n",
    "        self.mines = set(mines)\n",
    "        \n",
    "        self.slip_p      = slip_p\n",
    "        self.slip_mode   = slip_mode\n",
    "        self.portal_exit = portal_exit\n",
    "        \n",
    "        self.moving_obstacles = moving_obstacles\n",
    "        self.step_penalty  = step_penalty\n",
    "        self.mine_penalty  = mine_penalty\n",
    "        \n",
    "        # 6 possible actions in 3D\n",
    "        # 0: x+1, 1: x-1, 2: y+1, 3: y-1, 4: z+1, 5: z-1\n",
    "        self.actions = [0,1,2,3,4,5]\n",
    "        \n",
    "        self.time_step = 0  # for obstacle shifting\n",
    "\n",
    "    def _generate_random_obstacles(self, count=10):\n",
    "        \"\"\"Generate 'count' random obstacle positions (excluding start/goal).\"\"\"\n",
    "        obs = set()\n",
    "        possible_cells = []\n",
    "        for x in range(self.x_size):\n",
    "            for y in range(self.y_size):\n",
    "                for z in range(self.z_size):\n",
    "                    if (x,y,z) not in [self.start_state, self.goal_state]:\n",
    "                        possible_cells.append((x,y,z))\n",
    "        random.shuffle(possible_cells)\n",
    "        return set(possible_cells[:count])\n",
    "\n",
    "    def _generate_random_mines(self, count=5):\n",
    "        \"\"\"Generate 'count' random mines (excluding start, goal, obstacles).\"\"\"\n",
    "        mines = set()\n",
    "        possible_cells = []\n",
    "        for x in range(self.x_size):\n",
    "            for y in range(self.y_size):\n",
    "                for z in range(self.z_size):\n",
    "                    if ((x,y,z) != self.start_state and\n",
    "                        (x,y,z) != self.goal_state and\n",
    "                        (x,y,z) not in self.obstacles):\n",
    "                        possible_cells.append((x,y,z))\n",
    "        random.shuffle(possible_cells)\n",
    "        return set(possible_cells[:count])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start_state, time=0.\"\"\"\n",
    "        self.time_step = 0\n",
    "        return self.start_state\n",
    "\n",
    "    def step(self, state, action):\n",
    "        \"\"\"\n",
    "        step(...)\n",
    "        1) Move in 3D\n",
    "        2) Bound/Obstacle check\n",
    "        3) Slip check\n",
    "        4) Mines or goal => rewards\n",
    "        5) Possibly shift obstacles\n",
    "        returns next_state, reward, done\n",
    "        \"\"\"\n",
    "        (x, y, z) = state\n",
    "        # Movement logic\n",
    "        if action == 0:\n",
    "            nx, ny, nz = x+1, y, z\n",
    "        elif action == 1:\n",
    "            nx, ny, nz = x-1, y, z\n",
    "        elif action == 2:\n",
    "            nx, ny, nz = x, y+1, z\n",
    "        elif action == 3:\n",
    "            nx, ny, nz = x, y-1, z\n",
    "        elif action == 4:\n",
    "            nx, ny, nz = x, y, z+1\n",
    "        elif action == 5:\n",
    "            nx, ny, nz = x, y, z-1\n",
    "        else:\n",
    "            nx, ny, nz = x, y, z\n",
    "\n",
    "        # Bound check\n",
    "        if not self._in_bounds(nx, ny, nz):\n",
    "            nx, ny, nz = x, y, z\n",
    "        # Obstacle check\n",
    "        if (nx, ny, nz) in self.obstacles:\n",
    "            nx, ny, nz = x, y, z\n",
    "        \n",
    "        next_state = (nx, ny, nz)\n",
    "        \n",
    "        # Slip / portal\n",
    "        if random.random() < self.slip_p:\n",
    "            if self.slip_mode == 'random':\n",
    "                free_cells = []\n",
    "                for xx in range(self.x_size):\n",
    "                    for yy in range(self.y_size):\n",
    "                        for zz in range(self.z_size):\n",
    "                            if (xx,yy,zz) not in self.obstacles:\n",
    "                                free_cells.append((xx,yy,zz))\n",
    "                if free_cells:\n",
    "                    next_state = random.choice(free_cells)\n",
    "            else:  # 'fixed'\n",
    "                next_state = self.portal_exit\n",
    "        \n",
    "        # Reward setup\n",
    "        reward = self.step_penalty\n",
    "        done   = False\n",
    "        # Mine check\n",
    "        if next_state in self.mines:\n",
    "            reward += self.mine_penalty\n",
    "            done = True\n",
    "        # Goal check\n",
    "        elif next_state == self.goal_state:\n",
    "            reward += 1.0\n",
    "            done = True\n",
    "        \n",
    "        # Move obstacles over time\n",
    "        self.time_step += 1\n",
    "        if self.moving_obstacles:\n",
    "            self._update_obstacles()\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def _in_bounds(self, x, y, z):\n",
    "        return (0 <= x < self.x_size) and (0 <= y < self.y_size) and (0 <= z < self.z_size)\n",
    "\n",
    "    def _update_obstacles(self):\n",
    "        \"\"\"\n",
    "        Shift obstacles in +z each step (wrap around).\n",
    "        \"\"\"\n",
    "        new_obs = set()\n",
    "        for (ox, oy, oz) in self.obstacles:\n",
    "            newz = oz + 1\n",
    "            if newz >= self.z_size:\n",
    "                newz = 0\n",
    "            new_obs.add((ox, oy, newz))\n",
    "        self.obstacles = new_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "#                          HELPER FOR VALUE ITERATION\n",
    "###############################################################################\n",
    "def simulate_step_once(env, s, a):\n",
    "    \"\"\"\n",
    "    simulate_step_once\n",
    "    ------------------\n",
    "    This function replicates a single step without changing the real environment's \n",
    "    time or obstacles. It's used for approximate Value Iteration to sample transitions.\n",
    "    \"\"\"\n",
    "    (x,y,z) = s\n",
    "    if a == 0:\n",
    "        nx, ny, nz = x+1, y, z\n",
    "    elif a == 1:\n",
    "        nx, ny, nz = x-1, y, z\n",
    "    elif a == 2:\n",
    "        nx, ny, nz = x, y+1, z\n",
    "    elif a == 3:\n",
    "        nx, ny, nz = x, y-1, z\n",
    "    elif a == 4:\n",
    "        nx, ny, nz = x, y, z+1\n",
    "    elif a == 5:\n",
    "        nx, ny, nz = x, y, z-1\n",
    "    else:\n",
    "        nx, ny, nz = x,y,z\n",
    "\n",
    "    if not env._in_bounds(nx, ny, nz):\n",
    "        nx, ny, nz = x, y, z\n",
    "\n",
    "    if (nx, ny, nz) in env.obstacles:\n",
    "        nx, ny, nz = x, y, z\n",
    "\n",
    "    final_state = (nx, ny, nz)\n",
    "    # slip check\n",
    "    if random.random() < env.slip_p:\n",
    "        if env.slip_mode == 'random':\n",
    "            free_cells = []\n",
    "            for xx in range(env.x_size):\n",
    "                for yy in range(env.y_size):\n",
    "                    for zz in range(env.z_size):\n",
    "                        if (xx,yy,zz) not in env.obstacles:\n",
    "                            free_cells.append((xx,yy,zz))\n",
    "            if free_cells:\n",
    "                final_state = random.choice(free_cells)\n",
    "        else:\n",
    "            final_state = env.portal_exit\n",
    "\n",
    "    reward = env.step_penalty\n",
    "    done = False\n",
    "    if final_state in env.mines:\n",
    "        reward += env.mine_penalty\n",
    "        done = True\n",
    "    elif final_state == env.goal_state:\n",
    "        reward += 1.0\n",
    "        done = True\n",
    "    return final_state, reward, done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                            VALUE ITERATION 3D\n",
    "###############################################################################\n",
    "def value_iteration_3d(env, gamma=GAMMA, theta=THETA, max_iters=MAX_ITERS_VALUE_ITER, samples=SAMPLES_VALUE_ITER):\n",
    "    \"\"\"\n",
    "    value_iteration_3d\n",
    "    ------------------\n",
    "    Approximate Value Iteration for a 3D environment that might be nonstationary. \n",
    "    We sample multiple times to estimate transitions for each (s,a).\n",
    "    \n",
    "    Variables to experiment with:\n",
    "      - gamma: discount factor\n",
    "      - theta: convergence threshold\n",
    "      - max_iters: maximum iteration\n",
    "      - samples: how many times we sample each (s,a) for approximate distribution\n",
    "    \"\"\"\n",
    "    # Gather all states\n",
    "    states = []\n",
    "    for x in range(env.x_size):\n",
    "        for y in range(env.y_size):\n",
    "            for z in range(env.z_size):\n",
    "                states.append((x,y,z))\n",
    "    \n",
    "    # Initialize V\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0.0\n",
    "    \n",
    "    for iteration in range(max_iters):\n",
    "        delta = 0.0\n",
    "        for s in states:\n",
    "            if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "                continue\n",
    "            best_val = float('-inf')\n",
    "            for a in env.actions:\n",
    "                val_sum = 0.0\n",
    "                for _ in range(samples):\n",
    "                    ns, r, done = simulate_step_once(env, s, a)\n",
    "                    val_sum += r + (gamma*V[ns] if not done else 0.0)\n",
    "                avg_val = val_sum / samples\n",
    "                if avg_val > best_val:\n",
    "                    best_val = avg_val\n",
    "            old_v = V[s]\n",
    "            V[s] = best_val\n",
    "            delta = max(delta, abs(best_val - old_v))\n",
    "        if delta < theta:\n",
    "            print(f\"[ValueIteration3D] Converged at iteration={iteration} with delta={delta:.5f}\")\n",
    "            break\n",
    "    \n",
    "    # Derive policy\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "            policy[s] = None\n",
    "            continue\n",
    "        best_a = None\n",
    "        best_val = float('-inf')\n",
    "        for a in env.actions:\n",
    "            val_sum = 0.0\n",
    "            for _ in range(samples):\n",
    "                ns, r, done = simulate_step_once(env, s, a)\n",
    "                val_sum += r + (gamma*V[ns] if not done else 0.0)\n",
    "            avg_val = val_sum / samples\n",
    "            if avg_val > best_val:\n",
    "                best_val = avg_val\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                            MONTE CARLO (ON-POLICY)\n",
    "###############################################################################\n",
    "def mc_control_3d(env, gamma=GAMMA, epsilon=MC_EPSILON, episodes=MC_EPISODES):\n",
    "    \"\"\"\n",
    "    mc_control_3d\n",
    "    -------------\n",
    "    On-policy Monte Carlo with every-visit updates.\n",
    "\n",
    "    Variables to experiment with:\n",
    "      - gamma\n",
    "      - epsilon\n",
    "      - episodes\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "    returns_sum = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "    returns_count = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "\n",
    "    def epsilon_greedy(s):\n",
    "        if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "            return 0  # trivial action if terminal or blocked\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(env.actions)\n",
    "        else:\n",
    "            return np.argmax(Q[s])\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        episode = []\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = epsilon_greedy(s)\n",
    "            ns, r, done = env.step(s, a)\n",
    "            episode.append((s,a,r))\n",
    "            s = ns\n",
    "        \n",
    "        G = 0.0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s_t, a_t, r_t = episode[t]\n",
    "            G = gamma*G + r_t\n",
    "            if (s_t,a_t) not in visited:\n",
    "                visited.add((s_t,a_t))\n",
    "                returns_sum[s_t][a_t] += G\n",
    "                returns_count[s_t][a_t] += 1\n",
    "                Q[s_t][a_t] = returns_sum[s_t][a_t]/returns_count[s_t][a_t]\n",
    "    \n",
    "    # Extract policy\n",
    "    policy = {}\n",
    "    for x in range(env.x_size):\n",
    "        for y in range(env.y_size):\n",
    "            for z in range(env.z_size):\n",
    "                s = (x,y,z)\n",
    "                if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "                    policy[s] = None\n",
    "                else:\n",
    "                    policy[s] = np.argmax(Q[s])\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                            Q-LEARNING (TD)\n",
    "###############################################################################\n",
    "def q_learning_3d(env, alpha=QL_ALPHA, gamma=GAMMA, epsilon=QL_EPSILON, episodes=QL_EPISODES):\n",
    "    \"\"\"\n",
    "    q_learning_3d\n",
    "    -------------\n",
    "    Off-policy TD method to learn Q in a 3D environment.\n",
    "\n",
    "    Variables to experiment with:\n",
    "      - alpha (learning rate)\n",
    "      - gamma (discount)\n",
    "      - epsilon (exploration)\n",
    "      - episodes (# training episodes)\n",
    "    \"\"\"\n",
    "    Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "\n",
    "    def epsilon_greedy(s):\n",
    "        if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "            return 0\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(env.actions)\n",
    "        else:\n",
    "            return np.argmax(Q[s])\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = epsilon_greedy(s)\n",
    "            ns, r, done = env.step(s, a)\n",
    "            best_next_a = 0\n",
    "            if (ns not in env.obstacles) and (ns not in env.mines) and (ns != env.goal_state):\n",
    "                best_next_a = np.argmax(Q[ns])\n",
    "            \n",
    "            Q[s][a] += alpha * (r + gamma*Q[ns][best_next_a] - Q[s][a])\n",
    "            s = ns\n",
    "    \n",
    "    # Policy derivation\n",
    "    policy = {}\n",
    "    for x in range(env.x_size):\n",
    "        for y in range(env.y_size):\n",
    "            for z in range(env.z_size):\n",
    "                s = (x,y,z)\n",
    "                if s in env.obstacles or s in env.mines or s == env.goal_state:\n",
    "                    policy[s] = None\n",
    "                else:\n",
    "                    policy[s] = np.argmax(Q[s])\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running final 3D project code with the following parameters ---\n",
      "ENV_SIZE=(10, 10, 10), Start=(0, 0, 0), Goal=(9, 9, 9)\n",
      "OBSTACLES_COUNT=15, MINES_COUNT=5\n",
      "SLIP_P=0.2, SLIP_MODE=random\n",
      "MOVING_OBSTACLES=True, STEP_PENALTY=-0.01, MINE_PENALTY=-10.0\n",
      "Value Iteration => max_iters=60, samples=20\n",
      "MonteCarlo => episodes=4000, epsilon=0.15\n",
      "Q-Learning => episodes=4000, alpha=0.1, epsilon=0.15\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "=== Value Iteration (Approx) ===\n",
      "State (0, 0, 0): Value=0.109, Policy=0\n",
      "State (1, 1, 1): Value=0.101, Policy=4\n",
      "State (9, 9, 9): Value=0.0, Policy=None\n",
      "\n",
      "=== Monte Carlo (On-Policy) ===\n",
      "Sample policy at start state (0, 0, 0) => 4\n",
      "\n",
      "=== Q-Learning ===\n",
      "Sample policy at start state (0, 0, 0) => 4\n",
      "\n",
      "DONE. You can now analyze the final code and experiment with the top-level variables.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#                               MAIN SIMULATION\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    SIMULATION:\n",
    "    -----------\n",
    "    When you run this script, it will:\n",
    "\n",
    "    1) Create a 3D environment with your project-specific variables set at \n",
    "       the top of this file (ENV_SIZE, OBSTACLES_COUNT, MINES_COUNT, etc.)\n",
    "    2) Perform Value Iteration (approx) and print sample results\n",
    "    3) Perform Monte Carlo Control for MC_EPISODES episodes, then print \n",
    "       a sample policy action at start state\n",
    "    4) Perform Q-Learning for QL_EPISODES episodes, then also print a sample \n",
    "       policy action at start state\n",
    "\n",
    "    This demonstrates the final code for your project. \n",
    "    Modify the \"VARIABLES FOR EXPERIMENTATION\" section \n",
    "    to see how performance or behavior changes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack environment dimension\n",
    "    x_sz, y_sz, z_sz = ENV_SIZE\n",
    "\n",
    "    # Instantiate environment using the top-level variables\n",
    "    env_3d = Fancy3DGridEnv(\n",
    "        x_size=x_sz,\n",
    "        y_size=y_sz,\n",
    "        z_size=z_sz,\n",
    "        start_state=START_STATE,\n",
    "        goal_state=GOAL_STATE,\n",
    "        obstacles=None,   # random obstacles_count\n",
    "        mines=None,       # random mines_count\n",
    "        slip_p=SLIP_P,\n",
    "        slip_mode=SLIP_MODE,\n",
    "        portal_exit=PORTAL_EXIT,\n",
    "        moving_obstacles=MOVING_OBSTACLES,\n",
    "        step_penalty=STEP_PENALTY,\n",
    "        mine_penalty=MINE_PENALTY,\n",
    "        obstacles_count=OBSTACLES_COUNT,\n",
    "        mines_count=MINES_COUNT\n",
    "    )\n",
    "\n",
    "    # Print out the environment settings\n",
    "    print(\"\\n--- Running final 3D project code with the following parameters ---\")\n",
    "    print(f\"ENV_SIZE={ENV_SIZE}, Start={START_STATE}, Goal={GOAL_STATE}\")\n",
    "    print(f\"OBSTACLES_COUNT={OBSTACLES_COUNT}, MINES_COUNT={MINES_COUNT}\")\n",
    "    print(f\"SLIP_P={SLIP_P}, SLIP_MODE={SLIP_MODE}\")\n",
    "    print(f\"MOVING_OBSTACLES={MOVING_OBSTACLES}, STEP_PENALTY={STEP_PENALTY}, MINE_PENALTY={MINE_PENALTY}\")\n",
    "    print(f\"Value Iteration => max_iters={MAX_ITERS_VALUE_ITER}, samples={SAMPLES_VALUE_ITER}\")\n",
    "    print(f\"MonteCarlo => episodes={MC_EPISODES}, epsilon={MC_EPSILON}\")\n",
    "    print(f\"Q-Learning => episodes={QL_EPISODES}, alpha={QL_ALPHA}, epsilon={QL_EPSILON}\")\n",
    "    print(\"-----------------------------------------------------------------\\n\")\n",
    "\n",
    "    # 1) Value Iteration\n",
    "    print(\"=== Value Iteration (Approx) ===\")\n",
    "    V_3d, pi_3d = value_iteration_3d(\n",
    "        env_3d,\n",
    "        gamma=GAMMA,\n",
    "        theta=THETA,\n",
    "        max_iters=MAX_ITERS_VALUE_ITER,\n",
    "        samples=SAMPLES_VALUE_ITER\n",
    "    )\n",
    "    # Sample states\n",
    "    sample_check = [START_STATE, (1,1,1), GOAL_STATE]\n",
    "    for st in sample_check:\n",
    "        print(f\"State {st}: Value={round(V_3d[st],3)}, Policy={pi_3d[st]}\")\n",
    "\n",
    "    # 2) Monte Carlo\n",
    "    print(\"\\n=== Monte Carlo (On-Policy) ===\")\n",
    "    Q_mc, pi_mc = mc_control_3d(env_3d, gamma=GAMMA, epsilon=MC_EPSILON, episodes=MC_EPISODES)\n",
    "    print(f\"Sample policy at start state {START_STATE} => {pi_mc.get(START_STATE)}\")\n",
    "\n",
    "    # 3) Q-Learning\n",
    "    print(\"\\n=== Q-Learning ===\")\n",
    "    Q_q, pi_q = q_learning_3d(env_3d, alpha=QL_ALPHA, gamma=GAMMA, epsilon=QL_EPSILON, episodes=QL_EPISODES)\n",
    "    print(f\"Sample policy at start state {START_STATE} => {pi_q.get(START_STATE)}\")\n",
    "\n",
    "    print(\"\\nDONE. You can now analyze the final code and experiment with the top-level variables.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
